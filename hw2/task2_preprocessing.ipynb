{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "task2_preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOfX8UPRA7YJ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1.2: Word2vec preprocessing (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNy0lUlSA7YM",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing is not the most exciting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/text8.zip). For this task text preprocessing mostly consists of:\n",
        "\n",
        "1. cleaning (mostly, if your dataset is from social media or parsed from the internet)\n",
        "1. tokenization\n",
        "1. building the vocabulary and choosing its size. Use only high-frequency words, change all other words to UNK or handle it in your own manner. You can use `collections.Counter` for that.\n",
        "1. assigning each token a number (numericalization). In other words, make word2index и index2word objects.\n",
        "1. data structuring and batching - make X and y matrices generator for word2vec (explained in more details below)\n",
        "\n",
        "**ATTN!:** If you use your own data, please, attach a download link. \n",
        "\n",
        "Your goal is to make **Batcher** class which returns two numpy tensors with word indices. It should be possible to use one for word2vec training. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpful to remember the difference.\n",
        "\n",
        "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
        "\n",
        "There are several ways to do it right. Shapes could be `x_batch.shape = (batch_size, 2*window_size)`, `y_batch.shape = (batch_size,)` for CBOW or `(batch_size,)`, `(batch_size, 2*window_size)` for Skip-Gram. You should **not** do negative sampling here.\n",
        "\n",
        "They should be adequately parametrized: CBOW(window_size, ...), SkipGram(window_size, ...). You should implement only one batcher in this task; and it's up to you which one to chose.\n",
        "\n",
        "Useful links:\n",
        "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
        "\n",
        "You can write the code in this notebook, or in a separate file. It can be reused for the next task. The result of your work should represent that your batch has a proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
        "\n",
        "```\n",
        "text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "# CBOW:\n",
        "indices_to_words(x_batch) = \\\n",
        "        [['first', 'used', 'early', 'working'],\n",
        "        ['used', 'against', 'working', 'class'],\n",
        "        ['against', 'early', 'class', 'radicals'],\n",
        "        ['early', 'working', 'radicals', 'including']]\n",
        "\n",
        "indices_to_words(labels_batch) = ['against', 'early', 'working', 'class']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01lJrYDh6Mzt",
        "colab_type": "text"
      },
      "source": [
        "Building batcher for the skip-gram architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVicIvH2J2Gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, math, random, torch, collections, re\n",
        "import torch.nn as nn\n",
        "from pprint import pprint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUv6YXdxBe7J",
        "colab_type": "code",
        "outputId": "8856c90e-42a1-4b96-d654-364eddfc7a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip\n",
        "with open('text8') as text_file:\n",
        "    corpus = text_file.read().split()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-20 07:09:07--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M  2.18MB/s    in 14s     \n",
            "\n",
            "2020-02-20 07:09:26 (2.16 MB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8bcSgguqDr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "39d1a14c-0b43-46ad-cbd0-44c1463dfa01"
      },
      "source": [
        "pprint(' '.join(word for word in corpus[:100]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('anarchism originated as a term of abuse first used against early working '\n",
            " 'class radicals including the diggers of the english revolution and the sans '\n",
            " 'culottes of the french revolution whilst the term is still used in a '\n",
            " 'pejorative way to describe any act that used violent means to destroy the '\n",
            " 'organization of society it has also been taken up as a positive label by '\n",
            " 'self defined anarchists the word anarchism is derived from the greek without '\n",
            " 'archons ruler chief king anarchism as a political philosophy is the belief '\n",
            " 'that rulers are unnecessary and should be abolished although there are '\n",
            " 'differing')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0USDdLefNOLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCABULARY_SIZE = 10000\n",
        "UNK = '<UNK>'\n",
        "\n",
        "def create_dataset(corpus, vocab_size=VOCABULARY_SIZE, unk_token=UNK):\n",
        "    dataset = []\n",
        "    counter_dict = collections.Counter(corpus)\n",
        "    vocab = counter_dict.most_common(VOCABULARY_SIZE)\n",
        "    words = [x[0] for x in vocab]\n",
        "    words.append(unk_token)\n",
        "    min_allowed_freq = vocab[-1][1]\n",
        "    #use only high-frequency words\n",
        "    #change all other words to UNK\n",
        "    for _, word in enumerate(corpus):\n",
        "        if counter_dict[word] > min_allowed_freq:\n",
        "            dataset.append(word)\n",
        "        else:\n",
        "            dataset.append(unk_token)\n",
        "        \n",
        "    word2idx = {word: idx for (idx, word) in enumerate(words)}\n",
        "    idx2word = {idx: word for (idx, word) in enumerate(words)}\n",
        "    return dataset, word2idx, idx2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0yhrhi_kVfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, word2idx, idx2word = create_dataset(corpus=corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itr8TJ54A7YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batcher(object):\n",
        "    def __init__(self,dataset, window_size, batch_size, word2idx, idx2word):\n",
        "        self.dataset = dataset\n",
        "        self.window_size = window_size\n",
        "        self.batch_size = batch_size\n",
        "        self.word2idx = word2idx\n",
        "        self.idx2word = idx2word\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        global index\n",
        "        batch = []\n",
        "        labels = []\n",
        "        dataset = self.dataset \n",
        "        window_size = self.window_size\n",
        "        batch_size = self.batch_size\n",
        "        word2idx = self.word2idx\n",
        "        idx2word = self.idx2word\n",
        "        \n",
        "        for _ in range(batch_size):\n",
        "            # create a batch only if have at least\n",
        "            #n words to the left and n words to the right\n",
        "            #where n is window size\n",
        "            if (index - window_size < 0) or (index + window_size > len(dataset)-1):\n",
        "                #we need to increment index through epochs of learning\n",
        "                index = (index + 1) % len(dataset)\n",
        "            #now create context and batch\n",
        "            else:\n",
        "                #add word \n",
        "                batch.append(word2idx[dataset[index]])\n",
        "                \n",
        "                labels_batch = []\n",
        "\n",
        "                for word in dataset[index-window_size:index] + dataset[index+1: index+window_size+1]:\n",
        "                    labels_batch.append(word2idx[word])\n",
        "                labels.append(labels_batch)\n",
        "                #again update index\n",
        "                index = (index + 1) % len(dataset)\n",
        "        \n",
        "        return (batch, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_QBTLo2GpPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7e9be891-abac-4579-b0c3-dce564e84035"
      },
      "source": [
        "#time to test\n",
        "index = 0\n",
        "batcher = Batcher(dataset=data,window_size=3,batch_size=6,word2idx=word2idx,idx2word=idx2word)\n",
        "batches = iter(batcher)\n",
        "print('default corpus:', [d for d in data[:14]])\n",
        "batch, labels = next(batches)\n",
        "print(\"let's take a look at batches with window_size=\",window_size)\n",
        "print('batch', [idx2word[i] for i in batch], '\\n')\n",
        "print('labels:', [[idx2word[i] for i in x] for x in labels])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default corpus: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', '<UNK>']\n",
            "let's take a look at batches with window_size= 2\n",
            "batch ['a', 'term', 'of'] \n",
            "\n",
            "labels: [['anarchism', 'originated', 'as', 'term', 'of', 'abuse'], ['originated', 'as', 'a', 'of', 'abuse', 'first'], ['as', 'a', 'term', 'abuse', 'first', 'used']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCwc0MCzHcJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "fe647439-cb22-4bfb-94ca-82fa18ec859e"
      },
      "source": [
        "pprint(\" \".join(word for word in data[:100]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('the of and one in a to zero nine two is as eight for s five three was by '\n",
            " 'that four six seven with on are it from or his an be this which at he also '\n",
            " 'not have were has but other their its first they some had all more most can '\n",
            " 'been such many who new used there after when into american time these only '\n",
            " 'see may than world i b would d no however between about over years states '\n",
            " 'people war during united known if called use th system often state so '\n",
            " 'history will up while where')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}