{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "task1,2_character_lm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etomoscow/DL-in-NLP/blob/master/hw3/task1%2C2_character_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL599t9BBGF4",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2. Language modeling.\n",
        "\n",
        "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1. Character-based language modeling: data preparation (15 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_euJzO3BGF8",
        "colab_type": "text"
      },
      "source": [
        "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0qWIbjxBGF9",
        "colab_type": "code",
        "outputId": "35b86e2a-cfde-41e1-8c5a-28d26afabd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-23 11:21:46--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 533309 (521K) [text/plain]\n",
            "Saving to: ‘russian-train-high’\n",
            "\n",
            "russian-train-high  100%[===================>] 520.81K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-03-23 11:21:52 (10.1 MB/s) - ‘russian-train-high’ saved [533309/533309]\n",
            "\n",
            "--2020-03-23 11:21:54--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53671 (52K) [text/plain]\n",
            "Saving to: ‘russian-dev’\n",
            "\n",
            "russian-dev         100%[===================>]  52.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-03-23 11:21:54 (2.56 MB/s) - ‘russian-dev’ saved [53671/53671]\n",
            "\n",
            "--2020-03-23 11:21:56--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53514 (52K) [text/plain]\n",
            "Saving to: ‘russian-test’\n",
            "\n",
            "russian-test        100%[===================>]  52.26K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-03-23 11:21:56 (2.41 MB/s) - ‘russian-test’ saved [53514/53514]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEn-Va9cBGGC",
        "colab_type": "text"
      },
      "source": [
        "**1.1 (1 points)**\n",
        "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPl04joBGGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_infile(infile):\n",
        "    \"\"\"\n",
        "    implements simple data loader\n",
        "    \n",
        "    input: raw file \n",
        "    output: words (array-like)\n",
        "    \"\"\"\n",
        "    words, tags = [], []\n",
        "    with open(infile, 'r', encoding='utf-8') as ffile:\n",
        "        for string in ffile:\n",
        "            if len(string.strip().split('\\t')) !=3:\n",
        "                continue\n",
        "            words.append(string.strip().split('\\t')[0].lower())\n",
        "            tags.append(string.strip().split('\\t')[2])\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN0V2mVwBGGH",
        "colab_type": "code",
        "outputId": "be4ff013-a991-4900-c0a8-bc7ebb808110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_words = read_infile(\"russian-train-high\")\n",
        "dev_words = read_infile(\"russian-dev\")\n",
        "test_words = read_infile(\"russian-test\")\n",
        "print(len(train_words), len(dev_words), len(test_words))\n",
        "print(*train_words[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 1000 1000\n",
            "валлонский незаконченный истрёпывать личный серьга необоснованный тютя зарасти облётывать идеальный\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8TViTGxBGGL",
        "colab_type": "text"
      },
      "source": [
        "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PgbglT6BGGN",
        "colab_type": "code",
        "outputId": "1d18901c-745f-49a4-8e2f-34dd2bc91873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "PAD_token = '<PAD>'   # Used for padding short sentences\n",
        "BEGIN_token = '<BEGIN>'   # Start-of-sentence token\n",
        "END_token = '<END>'   # End-of-sentence token\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"\n",
        "    implements a simple vocabulary (thanks to kdnuggets.com)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def fit(self, data, show_sets=False):\n",
        "        terms = set(term for word in data for term in word)\n",
        "        self._terms = [PAD_token, END_token, BEGIN_token] + sorted(terms)\n",
        "        self.w2i = {word:ind for ind, word in enumerate(self._terms)}\n",
        "        self.i2w = {ind:word for ind, word in enumerate(self._terms)}\n",
        "        \n",
        "        if show_sets:\n",
        "            print('\\nTerms:', self._terms)\n",
        "            print('\\nWord2Index dict:', self.w2i)\n",
        "        return self\n",
        "\n",
        "    def __call__(self, data):\n",
        "        '''\n",
        "        applies .transform method to the given data\n",
        "\n",
        "        args:\n",
        "            data - input dataset\n",
        "        \n",
        "        output:\n",
        "            transformed dataset with indexes from Word2Index mapping\n",
        "        '''\n",
        "        transformed_data = [[self.w2i[term] for term in word] for word in data]\n",
        "        return transformed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._terms)\n",
        "    \n",
        "    def word2idx(self, word):\n",
        "        return self.w2i[word]\n",
        "    \n",
        "    def idx2word(self, ind):\n",
        "        return self.i2w[ind]\n",
        "\n",
        "vocab = Vocabulary()\n",
        "vocab.fit([list(x) for x in train_words], show_sets=True)\n",
        "print(len(vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Terms: ['<PAD>', '<END>', '<BEGIN>', ' ', '-', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
            "\n",
            "Word2Index dict: {'<PAD>': 0, '<END>': 1, '<BEGIN>': 2, ' ': 3, '-': 4, 'а': 5, 'б': 6, 'в': 7, 'г': 8, 'д': 9, 'е': 10, 'ж': 11, 'з': 12, 'и': 13, 'й': 14, 'к': 15, 'л': 16, 'м': 17, 'н': 18, 'о': 19, 'п': 20, 'р': 21, 'с': 22, 'т': 23, 'у': 24, 'ф': 25, 'х': 26, 'ц': 27, 'ч': 28, 'ш': 29, 'щ': 30, 'ъ': 31, 'ы': 32, 'ь': 33, 'э': 34, 'ю': 35, 'я': 36, 'ё': 37}\n",
            "38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNMk5z60dbq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_token = vocab.word2idx(PAD_token)\n",
        "END_token = vocab.word2idx(END_token)\n",
        "BEGIN_token = vocab.word2idx(BEGIN_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6jH0JjuBGGR",
        "colab_type": "text"
      },
      "source": [
        "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro8q7BnuBGGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "class Dataset(TorchDataset):\n",
        "    \n",
        "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
        "    def __init__(self, data, vocab, index=0, batch_size=1):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.index = index\n",
        "        self.batch_size = batch_size\n",
        "    def __getitem__(self, data):\n",
        "        \"\"\"\n",
        "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
        "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
        "        for the language model that obtain these word as input.        \n",
        "        \"\"\"\n",
        "        global index\n",
        "        data = self.data\n",
        "        batch_size = self.batch_size\n",
        "        batch = []\n",
        "        target = []\n",
        "        block = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            word = data[self.index]\n",
        "            word_t = vocab.__call__([word])[0]\n",
        "            target = word_t + [END_token]\n",
        "            word_t = [BEGIN_token] + word_t\n",
        "            \n",
        "            block.append((word_t, target, len(word)))\n",
        "            self.index = (self.index + 1) % len(data)\n",
        "            \n",
        "        block = sorted(block, key=lambda x: x[2], reverse=True)\n",
        "        real_batch = torch.FloatTensor([x[0] for x in block][0])\n",
        "        real_target = torch.FloatTensor([x[1] for x in block][0])\n",
        "        real_length = torch.FloatTensor([x[2] for x in block][0])\n",
        "        return (real_batch, real_target, real_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWMmzS_BBGGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 0 \n",
        "BATCH_SIZE = 1\n",
        "train_dataset = Dataset(train_words, vocab)\n",
        "dev_dataset = Dataset(dev_words, vocab)\n",
        "test_dataset = Dataset(test_words, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vkSHJNsFxvBe"
      },
      "source": [
        "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjpHghmWBGGc",
        "colab_type": "code",
        "outputId": "4db8b537-b686-4665-d835-4fdba6aa259b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader =  DataLoader(train_dataset, batch_size=1)\n",
        "for batch, i in zip(loader, range(10)):\n",
        "    print (batch[0].shape, batch[1].shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 11]) torch.Size([1, 11])\n",
            "torch.Size([1, 14]) torch.Size([1, 14])\n",
            "torch.Size([1, 12]) torch.Size([1, 12])\n",
            "torch.Size([1, 7]) torch.Size([1, 7])\n",
            "torch.Size([1, 7]) torch.Size([1, 7])\n",
            "torch.Size([1, 15]) torch.Size([1, 15])\n",
            "torch.Size([1, 5]) torch.Size([1, 5])\n",
            "torch.Size([1, 8]) torch.Size([1, 8])\n",
            "torch.Size([1, 11]) torch.Size([1, 11])\n",
            "torch.Size([1, 10]) torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jil04LkBGGg",
        "colab_type": "text"
      },
      "source": [
        "**(1.5) 1 point** Explain, why this does not work with larger batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HovMbjZbBGGh",
        "colab_type": "raw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbcqSTVmBGGi",
        "colab_type": "text"
      },
      "source": [
        "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1UQlEZBGGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tensor(vec, length, dim, pad_symbol=0):\n",
        "    \"\"\"\n",
        "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
        "    \"\"\"\n",
        "    dtype = torch.FloatTensor\n",
        "    #print('vec shape:',vec.shape,'len',length)\n",
        "    pad_size = list(vec.shape)\n",
        "    if length > vec.size(dim):\n",
        "        pad_size[dim] = length - vec.size(dim)\n",
        "        #print('pad_size',pad_size)\n",
        "        #print(torch.cat([vec.type(dtype), torch.zeros(*pad_size)], dim=dim))\n",
        "        return torch.cat([vec.type(dtype), torch.zeros(*pad_size)], dim=dim)\n",
        "    else:\n",
        "        return vec \n",
        "\n",
        "class Padder:\n",
        "    '''\n",
        "    padding tool, pads batches according to the needed shape\n",
        "    '''\n",
        "    def __init__(self, dim=0, pad_symbol=0):\n",
        "        '''\n",
        "        args:\n",
        "            dim - the dimension to be padded\n",
        "            pad_symbol - the symbol to be used in padding\n",
        "        '''\n",
        "        self.dim = dim\n",
        "        self.pad_symbol = pad_symbol\n",
        "        \n",
        "    def pad_collate(self, batch):\n",
        "        '''\n",
        "        args:\n",
        "            batch - list(batch, target)\n",
        "\n",
        "        return:\n",
        "            x - Tensor of all examples in 'batch' after padding\n",
        "            y - LongTensor of target in batch\n",
        "        '''\n",
        "        newbatch = []\n",
        "        max_length = max(list(map(lambda x: x[0].shape[self.dim], batch)))\n",
        "        for source, target, ln in batch:\n",
        "            newbatch.append((pad_tensor(source, max_length, self.dim),\n",
        "                             pad_tensor(target, max_length, self.dim),\n",
        "                             ln))\n",
        "        xs = torch.stack(list(map(lambda x: x[0], newbatch)), dim=0)\n",
        "        ys = torch.stack(list(map(lambda x: x[1], newbatch)), dim=0)\n",
        "        return xs, ys\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.pad_collate(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlWJMFqEBGGn",
        "colab_type": "text"
      },
      "source": [
        "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skz6yNqFBGGo",
        "colab_type": "code",
        "outputId": "274bf7d6-1260-4e52-fa34-78d70a8b062f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(train_dataset, batch_size=5, collate_fn=Padder(dim=0, pad_symbol=0))\n",
        "for batch, i in zip(loader, range(9)):\n",
        "    print(batch[0].shape, batch[1].shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 13]) torch.Size([5, 13])\n",
            "torch.Size([5, 12]) torch.Size([5, 12])\n",
            "torch.Size([5, 16]) torch.Size([5, 16])\n",
            "torch.Size([5, 18]) torch.Size([5, 18])\n",
            "torch.Size([5, 13]) torch.Size([5, 13])\n",
            "torch.Size([5, 14]) torch.Size([5, 14])\n",
            "torch.Size([5, 14]) torch.Size([5, 14])\n",
            "torch.Size([5, 14]) torch.Size([5, 14])\n",
            "torch.Size([5, 13]) torch.Size([5, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz-7RdpgBGGs",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Character-based language modeling. (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGNF_AnBGGt",
        "colab_type": "text"
      },
      "source": [
        "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
        "1. **Embedding** layer that transforms input symbols into vectors.\n",
        "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
        "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yUTOsz-BGGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embeddings_dim, hidden_dim, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embeddings_dim = embeddings_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.in_embs = nn.Embedding(vocab_size, embeddings_dim, padding_idx=PAD_token)\n",
        "        self.gru = nn.GRU(embeddings_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, inputs, hidden=None):\n",
        "        embs = self.in_embs(inputs)\n",
        "        packed = pack_padded_sequence(embs, length, batch_first=True)\n",
        "        out, f = self.gru(packed)\n",
        "        out, _ = pad_packed_sequence(out, batch_first=True)\n",
        "        out = self.output_layer(out)\n",
        "        scores = F.log_softmax(dim=-1)\n",
        "        return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-C3cKvBGGx",
        "colab_type": "text"
      },
      "source": [
        "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhAUDKDLBGGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_on_batch(model, criterion, x, y):\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMygPzahBGG2",
        "colab_type": "text"
      },
      "source": [
        "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6mrTTVuBGG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(model, criterion, x, y, optimizer):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xua7DlaBGG8",
        "colab_type": "text"
      },
      "source": [
        "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
        "\n",
        "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyiGlQpABGG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqut0-tTBGHA",
        "colab_type": "text"
      },
      "source": [
        "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPgsBgl5BGHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P1jcDwLBGHE",
        "colab_type": "text"
      },
      "source": [
        "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRfJhytBGHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdll2ayMBGHJ",
        "colab_type": "text"
      },
      "source": [
        "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me4WHjNjBGHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model, max_length=20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6rYQLbBGHN",
        "colab_type": "text"
      },
      "source": [
        "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42At-qHgBGHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNtcMi9BGHS",
        "colab_type": "text"
      },
      "source": [
        "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pX8m3JoBGHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(model, batch_size, max_length = 20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0QdG4hVBGHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated = []\n",
        "for _ in range(2):\n",
        "    generated += generate_batch(model, batch_size=10)\n",
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\"\n",
        "for elem in transformed:\n",
        "    print(\"\".join(elem))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXz5JmTtBGHY",
        "colab_type": "text"
      },
      "source": [
        "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaHSMq2-BGHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}