{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "task1,2_character_lm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etomoscow/DL-in-NLP/blob/master/hw3/task1%2C2_character_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL599t9BBGF4",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2. Language modeling.\n",
        "\n",
        "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1. Character-based language modeling: data preparation (15 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_euJzO3BGF8",
        "colab_type": "text"
      },
      "source": [
        "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0qWIbjxBGF9",
        "colab_type": "code",
        "outputId": "8ed94140-1763-4b53-e352-f862c204712b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-27 07:47:11--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 533309 (521K) [text/plain]\n",
            "Saving to: ‘russian-train-high’\n",
            "\n",
            "\rrussian-train-high    0%[                    ]       0  --.-KB/s               \rrussian-train-high  100%[===================>] 520.81K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-03-27 07:47:11 (13.4 MB/s) - ‘russian-train-high’ saved [533309/533309]\n",
            "\n",
            "--2020-03-27 07:47:14--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53671 (52K) [text/plain]\n",
            "Saving to: ‘russian-dev’\n",
            "\n",
            "russian-dev         100%[===================>]  52.41K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-27 07:47:14 (3.54 MB/s) - ‘russian-dev’ saved [53671/53671]\n",
            "\n",
            "--2020-03-27 07:47:16--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53514 (52K) [text/plain]\n",
            "Saving to: ‘russian-test’\n",
            "\n",
            "russian-test        100%[===================>]  52.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-27 07:47:16 (3.46 MB/s) - ‘russian-test’ saved [53514/53514]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEn-Va9cBGGC",
        "colab_type": "text"
      },
      "source": [
        "**1.1 (1 points)**\n",
        "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPl04joBGGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_infile(infile):\n",
        "    \"\"\"\n",
        "    implements simple data loader\n",
        "    \n",
        "    input: raw file \n",
        "    output: words (array-like)\n",
        "    \"\"\"\n",
        "    words, tags = [], []\n",
        "    with open(infile, 'r', encoding='utf-8') as ffile:\n",
        "        for string in ffile:\n",
        "            if len(string.strip().split('\\t')) !=3:\n",
        "                continue\n",
        "            words.append(string.strip().split('\\t')[0].lower())\n",
        "            tags.append(string.strip().split('\\t')[2])\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN0V2mVwBGGH",
        "colab_type": "code",
        "outputId": "7fed914a-36df-410b-c8f7-2dea2bcd7383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_words = read_infile(\"russian-train-high\")\n",
        "dev_words = read_infile(\"russian-dev\")\n",
        "test_words = read_infile(\"russian-test\")\n",
        "print(len(train_words), len(dev_words), len(test_words))\n",
        "print(*train_words[:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 1000 1000\n",
            "валлонский незаконченный истрёпывать личный серьга необоснованный тютя зарасти облётывать идеальный\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8TViTGxBGGL",
        "colab_type": "text"
      },
      "source": [
        "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PgbglT6BGGN",
        "colab_type": "code",
        "outputId": "5ff0dbc5-26c6-4110-ac54-8d8df80dfa30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "PAD_token = '<PAD>'   # Used for padding short sentences\n",
        "BEGIN_token = '<BEGIN>'   # Start-of-sentence token\n",
        "END_token = '<END>'   # End-of-sentence token\n",
        "\n",
        "class Vocabulary(object):\n",
        "    \"\"\"\n",
        "    implements a simple vocabulary (thanks to kdnuggets.com)\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def fit(self, data, show_sets=False):\n",
        "        terms = set(term for word in data for term in word)\n",
        "        self._terms = [PAD_token, END_token, BEGIN_token] + sorted(terms)\n",
        "        self.w2i = {word:ind for ind, word in enumerate(self._terms)}\n",
        "        self.i2w = {ind:word for ind, word in enumerate(self._terms)}\n",
        "        \n",
        "        if show_sets:\n",
        "            print('\\nTerms:', self._terms)\n",
        "            print('\\nWord2Index dict:', self.w2i)\n",
        "        return self\n",
        "\n",
        "    def __call__(self, data):\n",
        "        '''\n",
        "        applies .transform method to the given data\n",
        "\n",
        "        args:\n",
        "            data - input dataset\n",
        "        \n",
        "        output:\n",
        "            transformed dataset with indexes from Word2Index mapping\n",
        "        '''\n",
        "        transformed_data = [[self.w2i[term] for term in word] for word in data]\n",
        "        return transformed_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._terms)\n",
        "    \n",
        "    def word2idx(self, word):\n",
        "        return self.w2i[word]\n",
        "    \n",
        "    def idx2word(self, ind):\n",
        "        return self.i2w[ind]\n",
        "\n",
        "vocab = Vocabulary()\n",
        "vocab.fit([list(x) for x in train_words], show_sets=True)\n",
        "print(len(vocab))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Terms: ['<PAD>', '<END>', '<BEGIN>', ' ', '-', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n",
            "\n",
            "Word2Index dict: {'<PAD>': 0, '<END>': 1, '<BEGIN>': 2, ' ': 3, '-': 4, 'а': 5, 'б': 6, 'в': 7, 'г': 8, 'д': 9, 'е': 10, 'ж': 11, 'з': 12, 'и': 13, 'й': 14, 'к': 15, 'л': 16, 'м': 17, 'н': 18, 'о': 19, 'п': 20, 'р': 21, 'с': 22, 'т': 23, 'у': 24, 'ф': 25, 'х': 26, 'ц': 27, 'ч': 28, 'ш': 29, 'щ': 30, 'ъ': 31, 'ы': 32, 'ь': 33, 'э': 34, 'ю': 35, 'я': 36, 'ё': 37}\n",
            "38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNMk5z60dbq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_token = vocab.word2idx(PAD_token)\n",
        "END_token = vocab.word2idx(END_token)\n",
        "BEGIN_token = vocab.word2idx(BEGIN_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6jH0JjuBGGR",
        "colab_type": "text"
      },
      "source": [
        "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro8q7BnuBGGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "class Dataset(TorchDataset):\n",
        "    \n",
        "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
        "    def __init__(self, data, vocab, index=0, batch_size=1):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.index = index\n",
        "        self.batch_size = batch_size\n",
        "    def __getitem__(self, data):\n",
        "        \"\"\"\n",
        "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
        "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
        "        for the language model that obtain these word as input.        \n",
        "        \"\"\"\n",
        "        global index\n",
        "        data = self.data\n",
        "        batch_size = self.batch_size\n",
        "        batch = []\n",
        "        target = []\n",
        "        block = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            word = data[self.index]\n",
        "            word_t = vocab.__call__([word])[0]\n",
        "            target = word_t + [END_token]\n",
        "            word_t = [BEGIN_token] + word_t\n",
        "            \n",
        "            block.append((word_t, target, len(word)))\n",
        "            self.index = (self.index + 1) % len(data)\n",
        "            \n",
        "        block = sorted(block, key=lambda x: x[2], reverse=True)\n",
        "        real_batch = torch.FloatTensor([x[0] for x in block][0])\n",
        "        real_target = torch.FloatTensor([x[1] for x in block][0])\n",
        "        #real_length = torch.FloatTensor([x[2] for x in block][0])\n",
        "        return (real_batch, real_target)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWMmzS_BBGGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = 0 \n",
        "train_dataset = Dataset(train_words, vocab)\n",
        "dev_dataset = Dataset(dev_words, vocab)\n",
        "test_dataset = Dataset(test_words, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vkSHJNsFxvBe"
      },
      "source": [
        "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjpHghmWBGGc",
        "colab_type": "code",
        "outputId": "3c616f46-a664-4d38-9d2c-8ec8dac91dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader =  DataLoader(train_dataset, batch_size=1)\n",
        "for batch, i in zip(loader, range(10)):\n",
        "    print (batch[0].shape, batch[1].shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 11]) torch.Size([1, 11])\n",
            "torch.Size([1, 14]) torch.Size([1, 14])\n",
            "torch.Size([1, 12]) torch.Size([1, 12])\n",
            "torch.Size([1, 7]) torch.Size([1, 7])\n",
            "torch.Size([1, 7]) torch.Size([1, 7])\n",
            "torch.Size([1, 15]) torch.Size([1, 15])\n",
            "torch.Size([1, 5]) torch.Size([1, 5])\n",
            "torch.Size([1, 8]) torch.Size([1, 8])\n",
            "torch.Size([1, 11]) torch.Size([1, 11])\n",
            "torch.Size([1, 10]) torch.Size([1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jil04LkBGGg",
        "colab_type": "text"
      },
      "source": [
        "**(1.5) 1 point** Explain, why this does not work with larger batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HovMbjZbBGGh",
        "colab_type": "raw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbcqSTVmBGGi",
        "colab_type": "text"
      },
      "source": [
        "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1UQlEZBGGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tensor(vec, length, dim, pad_symbol=0):\n",
        "    \"\"\"\n",
        "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
        "    \"\"\"\n",
        "    dtype = torch.FloatTensor\n",
        "    #print('vec shape:',vec.shape,'len',length)\n",
        "    pad_size = list(vec.shape)\n",
        "    if length > vec.size(dim):\n",
        "        pad_size[dim] = length - vec.size(dim)\n",
        "        #print('pad_size',pad_size)\n",
        "        #print(torch.cat([vec.type(dtype), torch.zeros(*pad_size)], dim=dim))\n",
        "        return torch.cat([vec.type(dtype), torch.zeros(*pad_size)], dim=dim)\n",
        "    else:\n",
        "        return vec \n",
        "\n",
        "class Padder:\n",
        "    '''\n",
        "    padding tool, pads batches according to the needed shape\n",
        "    '''\n",
        "    def __init__(self, dim=0, pad_symbol=0):\n",
        "        '''\n",
        "        args:\n",
        "            dim - the dimension to be padded\n",
        "            pad_symbol - the symbol to be used in padding\n",
        "        '''\n",
        "        self.dim = dim\n",
        "        self.pad_symbol = pad_symbol\n",
        "        \n",
        "    def pad_collate(self, batch):\n",
        "        '''\n",
        "        args:\n",
        "            batch - list(batch, target)\n",
        "\n",
        "        return:\n",
        "            x - Tensor of all examples in 'batch' after padding\n",
        "            y - LongTensor of target in batch\n",
        "        '''\n",
        "        newbatch = []\n",
        "        max_length = max(list(map(lambda x: x[0].shape[self.dim], batch)))\n",
        "        for source, target in batch:\n",
        "            newbatch.append((pad_tensor(source, max_length, self.dim),\n",
        "                             pad_tensor(target, max_length, self.dim),\n",
        "                             max_length))\n",
        "            newbatch = sorted(newbatch, key= lambda x: x[2], reverse=True)\n",
        "        xs = torch.stack(list(map(lambda x: x[0], newbatch)), dim=0).long()\n",
        "        ys = torch.stack(list(map(lambda x: x[1], newbatch)), dim=0).long()\n",
        "        lens = torch.as_tensor([x[2] for x in newbatch], dtype=torch.int64)\n",
        "        return xs, ys, lens\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.pad_collate(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlWJMFqEBGGn",
        "colab_type": "text"
      },
      "source": [
        "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skz6yNqFBGGo",
        "colab_type": "code",
        "outputId": "8377e9e2-a8c7-4165-f94c-26e5449c7df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "index = 0\n",
        "loader = DataLoader(train_dataset, batch_size=5, collate_fn=Padder(dim=0, pad_symbol=0))\n",
        "for batch, i in zip(loader, range(9)):\n",
        "    if i==0:\n",
        "        print('batch tensor type:', batch[0].type(), batch[1].type())\n",
        "    print(batch[0].shape, batch[1].shape, batch[2])\n",
        "    "
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch tensor type: torch.LongTensor torch.LongTensor\n",
            "torch.Size([5, 17]) torch.Size([5, 17]) tensor([17, 17, 17, 17, 17])\n",
            "torch.Size([5, 13]) torch.Size([5, 13]) tensor([13, 13, 13, 13, 13])\n",
            "torch.Size([5, 15]) torch.Size([5, 15]) tensor([15, 15, 15, 15, 15])\n",
            "torch.Size([5, 43]) torch.Size([5, 43]) tensor([43, 43, 43, 43, 43])\n",
            "torch.Size([5, 16]) torch.Size([5, 16]) tensor([16, 16, 16, 16, 16])\n",
            "torch.Size([5, 17]) torch.Size([5, 17]) tensor([17, 17, 17, 17, 17])\n",
            "torch.Size([5, 12]) torch.Size([5, 12]) tensor([12, 12, 12, 12, 12])\n",
            "torch.Size([5, 14]) torch.Size([5, 14]) tensor([14, 14, 14, 14, 14])\n",
            "torch.Size([5, 23]) torch.Size([5, 23]) tensor([23, 23, 23, 23, 23])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz-7RdpgBGGs",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Character-based language modeling. (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIGNF_AnBGGt",
        "colab_type": "text"
      },
      "source": [
        "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
        "1. **Embedding** layer that transforms input symbols into vectors.\n",
        "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
        "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yUTOsz-BGGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device_cpu = torch.device('cpu')\n",
        "\n",
        "class RNNLM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embeddings_dim, hidden_dim, num_layers, dropout=0.):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embeddings_dim = embeddings_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.in_embs = nn.Embedding(vocab_size, embeddings_dim, padding_idx=PAD_token)\n",
        "        self.lstm = nn.LSTM(embeddings_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        embs = self.in_embs(inputs)\n",
        "        out, f = self.lstm(embs)\n",
        "        out = self.output_layer(out)\n",
        "        scores = F.log_softmax(out,dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        poeqgjopiwjOPIj\n",
        "        '''\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim, device = device), torch.zeros(self.num_layers, batch_size, self.hidden_dim, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-C3cKvBGGx",
        "colab_type": "text"
      },
      "source": [
        "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhAUDKDLBGGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_on_batch(model, criterion, x, y, batch_size, lens):\n",
        "    score = model.forward(x).to(device)\n",
        "    loss = criterion(scores.view(batch_size_ * max(lens), -1).to(dtype=torch.float32),\n",
        "                     y.view(batch_size*max(lens)))    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMygPzahBGG2",
        "colab_type": "text"
      },
      "source": [
        "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6mrTTVuBGG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(model, criterion, x, y, lens, optimizer):\n",
        "    \n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    batch_size_ = x.shape[0]\n",
        "\n",
        "    loss = validate_on_batch(model,criterion,x,y,batch_size_,lens)\n",
        "\n",
        "    return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xua7DlaBGG8",
        "colab_type": "text"
      },
      "source": [
        "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
        "\n",
        "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyiGlQpABGG9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd084967-32dc-4fc8-f3d0-bfcc0022357a"
      },
      "source": [
        "import math \n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = RNNLM(vocab_size=vocab.__len__(),\n",
        "              embeddings_dim=vocab.__len__(),\n",
        "              hidden_dim=256,\n",
        "              num_layers=2,\n",
        "              dropout=0.5).to(device)\n",
        "\n",
        "loss_func = nn.NLLLoss(reduction='sum', ignore_index=PAD_token)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "\n",
        "dev_perp = []\n",
        "train_perp = []\n",
        "losses_train = []\n",
        "losses_dev = []\n",
        "\n",
        "num_epochs = 500\n",
        "print_every = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sum_loss_train = 0\n",
        "    sum_loss_dev = 0\n",
        "    total_tokens_train = 0\n",
        "    total_tokens_dev = 0\n",
        "\n",
        "    \n",
        "    for batch_train, target_train, lens in DataLoader(train_dataset,\n",
        "                                                batch_size=100,\n",
        "                                                collate_fn=Padder(dim=0, pad_symbol=0)):\n",
        "        model.train()\n",
        "        batch_train = batch_train.to(device)\n",
        "        target_train = target_train.to(device)\n",
        "\n",
        "        batch_size_ = batch_train.shape[0]\n",
        "        batch_len = batch_train.shape[1]\n",
        "        scores = model.forward(batch_train).to(device)\n",
        "        loss = loss_func(scores.view(batch_size_*max(lens),-1).to(dtype=torch.float32), target_train.view(batch_size_*max(lens)))\n",
        "        sum_loss_train = sum_loss_train + loss.item()\n",
        "        total_tokens_train += float(batch_size_ + sum(lens))\n",
        "        \n",
        "        if model.training:\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        \n",
        "        perp_train = math.exp(sum_loss_train / total_tokens_train)\n",
        "        loss_train = sum_loss_train / total_tokens_train\n",
        "    losses_train.append(loss_train)\n",
        "    train_perp.append(perp_train)\n",
        "    \n",
        "    if epoch % print_every == 0:\n",
        "        print('Epoch: {}, loss:{}, perp:{}'.format(epoch, round(loss_train,4), round(perp_train,4)))\n",
        "\n",
        "    \n",
        "    for batch_eval, target_eval, lens in DataLoader(dev_dataset,\n",
        "                                                batch_size=10,\n",
        "                                                collate_fn=Padder(dim=0, pad_symbol=0)):\n",
        "        model.eval()\n",
        "        batch_eval = batch_eval.to(device=device)\n",
        "        target_eval = target_eval.to(device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_size_ = batch_eval.shape[0]\n",
        "            batch_len = batch_eval.shape[1]\n",
        "            scores = model.forward(batch_eval).to(device=device)\n",
        "            loss = loss_func(scores.view(batch_size_*max(lens), -1).to(dtype=torch.float32), target_eval.view(batch_size_*max(lens)))\n",
        "            sum_loss_dev = sum_loss_dev + loss.item()\n",
        "            total_tokens_dev += float(batch_size_ + sum(lens))\n",
        "\n",
        "        perp_dev = math.exp(sum_loss_dev / total_tokens_dev)\n",
        "        loss_dev = sum_loss_dev / total_tokens_dev\n",
        "    losses_dev.append(loss_dev)\n",
        "    dev_perp.append(perp_dev)\n",
        "    if epoch % print_every == 0:\n",
        "        print('epoch: {}, val loss:{}, val perp:{}'.format(epoch, round(loss_dev,4), round(perp_dev,4)))"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss:1.209, perp:3.35\n",
            "epoch: 0, val loss:1.5228, val perp:4.5853\n",
            "Epoch: 10, loss:0.9935, perp:2.7006\n",
            "epoch: 10, val loss:1.4345, val perp:4.1975\n",
            "Epoch: 20, loss:0.9823, perp:2.6706\n",
            "epoch: 20, val loss:1.4346, val perp:4.1981\n",
            "Epoch: 30, loss:0.9799, perp:2.6642\n",
            "epoch: 30, val loss:1.4362, val perp:4.2046\n",
            "Epoch: 40, loss:0.9797, perp:2.6637\n",
            "epoch: 40, val loss:1.4405, val perp:4.2229\n",
            "Epoch: 50, loss:0.9777, perp:2.6585\n",
            "epoch: 50, val loss:1.4452, val perp:4.2426\n",
            "Epoch: 60, loss:0.9795, perp:2.6632\n",
            "epoch: 60, val loss:1.4433, val perp:4.2347\n",
            "Epoch: 70, loss:0.9766, perp:2.6554\n",
            "epoch: 70, val loss:1.449, val perp:4.2587\n",
            "Epoch: 80, loss:0.9757, perp:2.653\n",
            "epoch: 80, val loss:1.449, val perp:4.2587\n",
            "Epoch: 90, loss:0.977, perp:2.6565\n",
            "epoch: 90, val loss:1.4521, val perp:4.2721\n",
            "Epoch: 100, loss:0.9764, perp:2.6549\n",
            "epoch: 100, val loss:1.4519, val perp:4.2714\n",
            "Epoch: 110, loss:0.9762, perp:2.6544\n",
            "epoch: 110, val loss:1.4545, val perp:4.2824\n",
            "Epoch: 120, loss:0.9754, perp:2.6521\n",
            "epoch: 120, val loss:1.457, val perp:4.2929\n",
            "Epoch: 130, loss:0.9739, perp:2.6483\n",
            "epoch: 130, val loss:1.4608, val perp:4.3093\n",
            "Epoch: 140, loss:0.9753, perp:2.6519\n",
            "epoch: 140, val loss:1.4628, val perp:4.3181\n",
            "Epoch: 150, loss:0.9745, perp:2.6499\n",
            "epoch: 150, val loss:1.4667, val perp:4.3349\n",
            "Epoch: 160, loss:0.975, perp:2.6511\n",
            "epoch: 160, val loss:1.4711, val perp:4.3541\n",
            "Epoch: 170, loss:0.973, perp:2.646\n",
            "epoch: 170, val loss:1.4736, val perp:4.3648\n",
            "Epoch: 180, loss:0.9738, perp:2.648\n",
            "epoch: 180, val loss:1.4704, val perp:4.3511\n",
            "Epoch: 190, loss:0.9747, perp:2.6505\n",
            "epoch: 190, val loss:1.4735, val perp:4.3647\n",
            "Epoch: 200, loss:0.9752, perp:2.6518\n",
            "epoch: 200, val loss:1.4777, val perp:4.3829\n",
            "Epoch: 210, loss:0.9731, perp:2.6462\n",
            "epoch: 210, val loss:1.4766, val perp:4.3779\n",
            "Epoch: 220, loss:0.9752, perp:2.6516\n",
            "epoch: 220, val loss:1.4717, val perp:4.3565\n",
            "Epoch: 230, loss:0.9739, perp:2.6481\n",
            "epoch: 230, val loss:1.479, val perp:4.3885\n",
            "Epoch: 240, loss:0.977, perp:2.6565\n",
            "epoch: 240, val loss:1.4764, val perp:4.3772\n",
            "Epoch: 250, loss:0.9743, perp:2.6494\n",
            "epoch: 250, val loss:1.4831, val perp:4.4067\n",
            "Epoch: 260, loss:0.9821, perp:2.6702\n",
            "epoch: 260, val loss:1.4811, val perp:4.3977\n",
            "Epoch: 270, loss:0.9776, perp:2.6579\n",
            "epoch: 270, val loss:1.4801, val perp:4.3934\n",
            "Epoch: 280, loss:0.9768, perp:2.6558\n",
            "epoch: 280, val loss:1.4824, val perp:4.4034\n",
            "Epoch: 290, loss:0.9766, perp:2.6555\n",
            "epoch: 290, val loss:1.4873, val perp:4.425\n",
            "Epoch: 300, loss:0.9752, perp:2.6518\n",
            "epoch: 300, val loss:1.4931, val perp:4.4508\n",
            "Epoch: 310, loss:0.9759, perp:2.6535\n",
            "epoch: 310, val loss:1.4966, val perp:4.4663\n",
            "Epoch: 320, loss:0.9745, perp:2.6498\n",
            "epoch: 320, val loss:1.495, val perp:4.4594\n",
            "Epoch: 330, loss:0.9738, perp:2.6479\n",
            "epoch: 330, val loss:1.4942, val perp:4.456\n",
            "Epoch: 340, loss:0.9736, perp:2.6476\n",
            "epoch: 340, val loss:1.496, val perp:4.464\n",
            "Epoch: 350, loss:0.9713, perp:2.6414\n",
            "epoch: 350, val loss:1.5005, val perp:4.4838\n",
            "Epoch: 360, loss:0.9713, perp:2.6415\n",
            "epoch: 360, val loss:1.5068, val perp:4.5123\n",
            "Epoch: 370, loss:0.971, perp:2.6407\n",
            "epoch: 370, val loss:1.5083, val perp:4.519\n",
            "Epoch: 380, loss:0.9696, perp:2.6368\n",
            "epoch: 380, val loss:1.5174, val perp:4.5605\n",
            "Epoch: 390, loss:0.9696, perp:2.6369\n",
            "epoch: 390, val loss:1.5174, val perp:4.5605\n",
            "Epoch: 400, loss:0.9685, perp:2.6339\n",
            "epoch: 400, val loss:1.5257, val perp:4.5985\n",
            "Epoch: 410, loss:0.9673, perp:2.6309\n",
            "epoch: 410, val loss:1.5345, val perp:4.6392\n",
            "Epoch: 420, loss:0.9678, perp:2.6322\n",
            "epoch: 420, val loss:1.5355, val perp:4.6434\n",
            "Epoch: 430, loss:0.9689, perp:2.6351\n",
            "epoch: 430, val loss:1.5417, val perp:4.6727\n",
            "Epoch: 440, loss:0.9689, perp:2.635\n",
            "epoch: 440, val loss:1.5377, val perp:4.654\n",
            "Epoch: 450, loss:0.9685, perp:2.634\n",
            "epoch: 450, val loss:1.534, val perp:4.6365\n",
            "Epoch: 460, loss:0.97, perp:2.638\n",
            "epoch: 460, val loss:1.5337, val perp:4.6355\n",
            "Epoch: 470, loss:0.9767, perp:2.6558\n",
            "epoch: 470, val loss:1.5339, val perp:4.636\n",
            "Epoch: 480, loss:0.9774, perp:2.6574\n",
            "epoch: 480, val loss:1.5326, val perp:4.6303\n",
            "Epoch: 490, loss:0.9774, perp:2.6576\n",
            "epoch: 490, val loss:1.5263, val perp:4.6013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGM3OsP6bfP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "c51b74c4-dc2e-4af0-9e99-b800661ebabe"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.figure(figsize=(12,7))\n",
        "plt.title('Average losses per epoch')\n",
        "plt.plot(losses_train, label='train')\n",
        "plt.plot(losses_dev, label='dev')\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0a100d3588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAGrCAYAAADKAfHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqut0-tTBGHA",
        "colab_type": "text"
      },
      "source": [
        "=**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPgsBgl5BGHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "def predict_on_batch(loader, model, n=20):\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    probabilities = []\n",
        "    ii = 0\n",
        "    for x, y, lens in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        batch_size = x.shape[0]\n",
        "        model.eval()\n",
        "        scores = model.forward(x)[0].to(device='cpu').detach().numpy()\n",
        "        y = y.to(device='cpu')\n",
        "        score_e = np.exp(scores)\n",
        "        y_w = [vocab.idx2word(i) for i in y[0].detach().numpy()]\n",
        "        pred_w = [vocab.idx2word(i) for i in np.argmax(scores, axis=1)]\n",
        "        temp = []\n",
        "        for j in range(len(scores)):\n",
        "            temp.append(math.exp(scores[j, y[0,j]]))\n",
        "        probabilities.append((np.prod(temp),\n",
        "                             lens[0],\n",
        "                             1 / vocab.__len__()**lens[0]))\n",
        "        temp = np.round(temp, 2)\n",
        "        out_df = pd.DataFrame({i : [pred_w[i], y_w[i], temp[i]] for i,_ in enumerate(temp)})\n",
        "        if ii > len(loader)-1-n:\n",
        "            print('-'*70)\n",
        "            print(out_df)\n",
        "            print('\\nProbs: {}\\t word len: {}'.format(probabilities[-1][0],\n",
        "                                                                          probabilities[-1][1]))\n",
        "            print('-'*70)\n",
        "        ii += 1\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P1jcDwLBGHE",
        "colab_type": "text"
      },
      "source": [
        "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRfJhytBGHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4db0142c-0fd9-4263-9b46-f90d2b812697"
      },
      "source": [
        "trainloader = DataLoader(train_dataset,\n",
        "                    batch_size=1,\n",
        "                    collate_fn=Padder(dim=0, pad_symbol=0), shuffle=False)\n",
        "predict_on_batch(loader=trainloader, model=model)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "      0     1  2     3     4     5     6     7      8\n",
            "0     к     т  е     а     т     а     ы     й  <END>\n",
            "1     а     л  м     а     з     н     ы     й  <END>\n",
            "2  0.06  0.04  0  0.38  0.02  0.04  0.35  0.79   0.84\n",
            "\n",
            "Probs: 3.003575362002795e-10\t word len: 9\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3  4     5    6     7      8\n",
            "0     к     ь     т     а  в     а    ы     т  <END>\n",
            "1     т     а     м     о  ш     н    и     й  <END>\n",
            "2  0.07  0.17  0.11  0.07  0  0.04  0.1  0.09   0.84\n",
            "\n",
            "Probs: 1.0570132673228715e-10\t word len: 9\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "     0     1     2     3    4    5     6     7     8     9      10\n",
            "0     к     е     т     а    ы    т     а     в     а     й  <END>\n",
            "1     л     е     д     н    и    к     о     в     ы     й  <END>\n",
            "2  0.01  0.23  0.07  0.04  0.1  0.1  0.03  0.14  0.05  0.79   0.84\n",
            "\n",
            "Probs: 1.0037449271630871e-11\t word len: 11\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0  1     2     3      4  5      6\n",
            "0     к  к     т     е  <END>  а  <END>\n",
            "1     с  е     л     ь      д  ь  <END>\n",
            "2  0.05  0  0.05  0.23      0  0   0.79\n",
            "\n",
            "Probs: 8.49772458661728e-14\t word len: 7\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "     0     1     2     3      4     5     6     7      8     9      10\n",
            "0     к     к     ь     о  <END>     к     т     ь  <END>     к  <END>\n",
            "1     с     т     р     я      с     а     т     ь      с     я  <END>\n",
            "2  0.05  0.17  0.14  0.01   0.01  0.15  0.21  0.24   0.08  0.08   0.39\n",
            "\n",
            "Probs: 1.8244575130451676e-12\t word len: 11\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3     4     5      6\n",
            "0     к     а     в     а     ы     й  <END>\n",
            "1     ч     у     д     н     ы     й  <END>\n",
            "2  0.03  0.01  0.05  0.04  0.35  0.79   0.84\n",
            "\n",
            "Probs: 1.5220992255899073e-07\t word len: 7\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1  2  3     4     5     6     7     8      9\n",
            "0     к     а  т  а     в     к     ь     т     ь  <END>\n",
            "1     з     а  г  у     с     т     е     т     ь  <END>\n",
            "2  0.04  0.27  0  0  0.07  0.17  0.05  0.13  0.24   0.79\n",
            "\n",
            "Probs: 1.4949645580609508e-14\t word len: 10\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "    0     1     2     3     4     5     6     7     8     9     10 11     12\n",
            "0    к     а     о     т     а     в     к     ь     т     о     т  а  <END>\n",
            "1    п     р     е     д     о     с     т     е     р     е     ч  ь  <END>\n",
            "2  0.1  0.15  0.15  0.07  0.12  0.07  0.17  0.05  0.06  0.15  0.06  0   0.79\n",
            "\n",
            "Probs: 1.0976927620374878e-14\t word len: 13\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2  3     4     5     6     7      8\n",
            "0     к     к     т  ь     а     т     а     й  <END>\n",
            "1     с     и     т  ц     е     в     ы     й  <END>\n",
            "2  0.05  0.06  0.15  0  0.14  0.08  0.05  0.79   0.84\n",
            "\n",
            "Probs: 2.394869667658287e-11\t word len: 9\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "    0     1     2     3     4     5  6     7     8     9      10\n",
            "0    к     а     в     а     а     т  о     а     т     ь  <END>\n",
            "1    п     о     д     в     е     р  г     а     т     ь  <END>\n",
            "2  0.1  0.18  0.05  0.03  0.14  0.06  0  0.21  0.21  0.24   0.79\n",
            "\n",
            "Probs: 4.880817205851622e-13\t word len: 11\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3     4     5      6\n",
            "0     к     а     в     о     а     т  <END>\n",
            "1     г     о     р     ш     и     й  <END>\n",
            "2  0.05  0.18  0.08  0.01  0.15  0.09   0.84\n",
            "\n",
            "Probs: 5.973974119703574e-08\t word len: 7\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3     4      5\n",
            "0     к     а     т     о     т  <END>\n",
            "1     к     а     р     и     й  <END>\n",
            "2  0.11  0.56  0.05  0.13  0.09   0.84\n",
            "\n",
            "Probs: 3.0792412531266294e-05\t word len: 6\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0  1     2     3     4     5     6     7      8\n",
            "0     к  т     в     о     в     к     ь     т      ь\n",
            "1     а  э     р     о     с     т     а     т  <END>\n",
            "2  0.06  0  0.08  0.21  0.07  0.17  0.17  0.21   0.07\n",
            "\n",
            "Probs: 3.115739626362702e-13\t word len: 9\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "     0     1     2     3     4     5     6     7     8     9     10     11\n",
            "0     к     в     ь     к     о     в     а     т     а     т     ь  <END>\n",
            "1     о     т     с     р     о     ч     и     в     а     т     ь  <END>\n",
            "2  0.07  0.08  0.02  0.04  0.21  0.06  0.15  0.13  0.27  0.21  0.24   0.79\n",
            "\n",
            "Probs: 1.2179065324495101e-11\t word len: 12\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3    4     5     6     7     8      9\n",
            "0     к     т     о     в    ы     т     а     ы     й  <END>\n",
            "1     и     р     о     н    и     ч     н     ы     й  <END>\n",
            "2  0.02  0.04  0.21  0.05  0.1  0.06  0.04  0.35  0.79   0.84\n",
            "\n",
            "Probs: 5.690727605916383e-10\t word len: 10\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "     0      1     2     3  4     5     6  7     8     9     10     11\n",
            "0     к  <END>     в     о  а     а     т  о     к     а     т  <END>\n",
            "1     й      о     р     к  ш     и     р  с     к     и     й  <END>\n",
            "2  0.01      0  0.08  0.02  0  0.15  0.04  0  0.18  0.05  0.09   0.84\n",
            "\n",
            "Probs: 2.354301468309477e-23\t word len: 12\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1  2     3     4    5     6    7    8      9\n",
            "0     к     т  а     а     ы    т     ы    ы    т      а\n",
            "1     и     з  г     н     а    н     н    и    к  <END>\n",
            "2  0.02  0.02  0  0.02  0.04  0.1  0.03  0.1  0.1   0.04\n",
            "\n",
            "Probs: 2.2419147575388376e-17\t word len: 10\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "     0     1     2     3     4     5   ...    7     8     9     10    11     12\n",
            "0     к     а     в     а     в     о  ...     а     т     ы     ы     й  <END>\n",
            "1     д     о     м     о     р     о  ...     е     н     н     ы     й  <END>\n",
            "2  0.02  0.12  0.02  0.07  0.08  0.21  ...  0.15  0.11  0.03  0.35  0.79   0.84\n",
            "\n",
            "[3 rows x 13 columns]\n",
            "\n",
            "Probs: 3.0140878972323135e-14\t word len: 13\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "      0     1     2     3     4     5     6     7      8\n",
            "0     к     ы     т     а     ы     в     а     т  <END>\n",
            "1     н     е     м     н     о     г     и     й  <END>\n",
            "2  0.04  0.07  0.03  0.03  0.13  0.01  0.13  0.09   0.84\n",
            "\n",
            "Probs: 4.16118831868601e-11\t word len: 9\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "  0     1     2     3     4  5   ...    8     9      10    11    12     13\n",
            "0  к     в     а     в     а  т  ...     т     е  <END>     ы     й  <END>\n",
            "1  э     м     о     ц     и  о  ...     л     ь      н     ы     й  <END>\n",
            "2  0  0.02  0.07  0.01  0.16  0  ...  0.04  0.23   0.11  0.35  0.79   0.84\n",
            "\n",
            "[3 rows x 14 columns]\n",
            "\n",
            "Probs: 5.476867984200553e-22\t word len: 14\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtE1h44506VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdll2ayMBGHJ",
        "colab_type": "text"
      },
      "source": [
        "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me4WHjNjBGHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model, max_length=20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS6rYQLbBGHN",
        "colab_type": "text"
      },
      "source": [
        "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42At-qHgBGHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNtcMi9BGHS",
        "colab_type": "text"
      },
      "source": [
        "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pX8m3JoBGHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(model, batch_size, max_length = 20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0QdG4hVBGHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated = []\n",
        "for _ in range(2):\n",
        "    generated += generate_batch(model, batch_size=10)\n",
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\"\n",
        "for elem in transformed:\n",
        "    print(\"\".join(elem))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXz5JmTtBGHY",
        "colab_type": "text"
      },
      "source": [
        "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaHSMq2-BGHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}