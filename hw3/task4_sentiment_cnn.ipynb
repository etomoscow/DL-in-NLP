{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "task4_sentiment_cnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etomoscow/DL-in-NLP/blob/master/hw3/task4_sentiment_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNKbmhbwU-fS",
        "colab_type": "text"
      },
      "source": [
        "## Assignment 2.4: Text classification via CNN (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXe3uSABU-fT",
        "colab_type": "text"
      },
      "source": [
        "In this assignment you should perform sentiment analysis of the IMDB reviews based on CNN architecture. Read carefully [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf) by Yoon Kim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUjT2NBbU-fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score \n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import BucketIterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-aB34JbU-fY",
        "colab_type": "text"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44NyMyYrU-fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = Field(sequential=True,\n",
        "             lower=True,\n",
        "             batch_first=True,\n",
        "             fix_length=200,\n",
        "             include_lengths=True\n",
        "             )\n",
        "LABEL = LabelField(batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8PjFm8nU-fb",
        "colab_type": "code",
        "outputId": "41fbcd3f-5f9b-4365-e053-200b0fa15265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
        "trn, vld = train.split()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W32nW_RKU-fe",
        "colab_type": "code",
        "outputId": "6538c80d-c2a9-4889-a00e-7f96b375b392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "TEXT.build_vocab(trn)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.14 s, sys: 30.9 ms, total: 1.17 s\n",
            "Wall time: 1.18 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rQIS06DU-fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(trn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LBcvS2PU-fl",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Iterator (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFcQdUPBU-fm",
        "colab_type": "text"
      },
      "source": [
        "Define an iterator here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfLcZ8twU-fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "        (trn, vld, tst),\n",
        "        batch_sizes=(64, 64, 64),\n",
        "        sort=False,\n",
        "        sort_key= lambda x: len(x.text),\n",
        "        sort_within_batch=False,\n",
        "        device='cuda',\n",
        "        repeat=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrIWvjv-b_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(train_iter.__iter__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5peD6GdVU-fp",
        "colab_type": "text"
      },
      "source": [
        "### Define CNN-based text classification model (8 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ChqeI3cU-fq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, V, D, kernel_sizes, stride=1, padding=0, dropout=0.5, out_channels=1):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.V = V\n",
        "        self.D = D\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels\n",
        "    \n",
        "        self.embs = nn.Embedding(V, D, padding_idx=1)\n",
        "        self.conv1 = nn.Conv2d(1, out_channels,(kernel_sizes[0], D))\n",
        "        self.conv2 = nn.Conv2d(1, out_channels, (kernel_sizes[1], D))\n",
        "        self.conv3 = nn.Conv2d(1, out_channels, (kernel_sizes[2], D))\n",
        "        self.dp = nn.Dropout(dropout)\n",
        "        self.lin = nn.Linear(len(kernel_sizes)*out_channels, 2)\n",
        "    \n",
        "    \n",
        "    def convolution_block(self, input, conv):\n",
        "        conved = conv(input)\n",
        "        act = F.relu(conved.squeeze(3))\n",
        "        out = F.max_pool1d(act, act.size()[2]).squeeze(2)\n",
        "\n",
        "        return out\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embs(x).unsqueeze(1)\n",
        "        out1 = self.convolution_block(x, self.conv1)\n",
        "        out2 = self.convolution_block(x, self.conv2)\n",
        "        out3 = self.convolution_block(x, self.conv3)\n",
        "\n",
        "        concat = self.dp(torch.cat((out1, out2, out3), 1))\n",
        "        logit = self.lin(concat)\n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBZiWL2gU-fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_sizes = [3,4,5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 300\n",
        "\n",
        "model = CNN(vocab_size, dim, kernel_sizes, dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTK0EO7zU-fv",
        "colab_type": "code",
        "outputId": "62de2dea-b791-420d-b90d-6040db9173f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embs): Embedding(201765, 300, padding_idx=1)\n",
              "  (conv1): Conv2d(1, 1, kernel_size=(3, 300), stride=(1, 1))\n",
              "  (conv2): Conv2d(1, 1, kernel_size=(4, 300), stride=(1, 1))\n",
              "  (conv3): Conv2d(1, 1, kernel_size=(5, 300), stride=(1, 1))\n",
              "  (dp): Dropout(p=0.5, inplace=False)\n",
              "  (lin): Linear(in_features=3, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPFSMvIfU-fy",
        "colab_type": "text"
      },
      "source": [
        "### The training loop (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvSQD1ZVU-fy",
        "colab_type": "text"
      },
      "source": [
        "Define the optimization function and the loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFk48mD3U-fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = optim.Adam(model.parameters(), lr=0.05)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smv08lRPU-f2",
        "colab_type": "text"
      },
      "source": [
        "Think carefully about the stopping criteria. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLOeQhVjU-f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ickPlowMal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, optimizer, loss_func, num_epochs, trainset, valset):\n",
        "    %%time\n",
        "    train_loss, valid_loss = [], []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_acc, val_acc = [], []\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        model.train() \n",
        "        for batch in trainset:         \n",
        "        \n",
        "            x = batch.text[0]\n",
        "            y = batch.label\n",
        "        \n",
        "            opt.zero_grad()\n",
        "            preds = model(x)\n",
        "            loss = loss_func(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "            n_correct = (torch.max(preds, 1)[1].view(y.size()).data == y.data).float().sum().detach().cpu().numpy()\n",
        "            train_acc.append(100.0 * n_correct/len(batch))\n",
        "\n",
        "        epoch_loss = running_loss / len(trn)\n",
        "        train_loss.append(epoch_loss)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0 \n",
        "        for batch in val_iter:\n",
        "            with torch.no_grad():\n",
        "                x = batch.text[0]\n",
        "                y = batch.label\n",
        "        \n",
        "                preds = model(x)\n",
        "                loss = loss_func(preds, y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                n_correct = (torch.max(preds, 1)[1].view(y.size()).data == y.data).float().sum().detach().cpu().numpy()\n",
        "                val_acc.append(100.0 * n_correct/len(batch))\n",
        "\n",
        "        val_loss /= len(vld)\n",
        "        valid_loss.append(val_loss)\n",
        "        print('Epoch: {}, Training Loss: {}, Training Accuracy: {}, Validation Loss: {}, Validation Accuracy: {}'.format(epoch, round(epoch_loss, 5), round(np.mean(train_acc), 2), round(val_loss, 5), round(np.mean(val_acc), 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiDeGdcSwxFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9709a8c0-a1b7-442b-f0c1-7ffd1f43d5b3"
      },
      "source": [
        "train_model(model, opt, loss_func, epochs, train_iter, val_iter)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.77 µs\n",
            "Epoch: 1, Training Loss: 0.01247, Training Accuracy: 49.77, Validation Loss: 0.01126, Validation Accuracy: 49.83\n",
            "Epoch: 2, Training Loss: 0.01205, Training Accuracy: 50.75, Validation Loss: 0.01096, Validation Accuracy: 48.63\n",
            "Epoch: 3, Training Loss: 0.01219, Training Accuracy: 52.93, Validation Loss: 0.01172, Validation Accuracy: 53.03\n",
            "Epoch: 4, Training Loss: 0.01445, Training Accuracy: 53.81, Validation Loss: 0.01283, Validation Accuracy: 51.22\n",
            "Epoch: 5, Training Loss: 0.01604, Training Accuracy: 54.01, Validation Loss: 0.01227, Validation Accuracy: 50.45\n",
            "Epoch: 6, Training Loss: 0.01544, Training Accuracy: 53.99, Validation Loss: 0.01374, Validation Accuracy: 51.64\n",
            "Epoch: 7, Training Loss: 0.01543, Training Accuracy: 54.58, Validation Loss: 0.01923, Validation Accuracy: 49.83\n",
            "Epoch: 8, Training Loss: 0.01565, Training Accuracy: 55.7, Validation Loss: 0.01253, Validation Accuracy: 52.28\n",
            "Epoch: 9, Training Loss: 0.0172, Training Accuracy: 56.11, Validation Loss: 0.01353, Validation Accuracy: 52.69\n",
            "Epoch: 10, Training Loss: 0.01351, Training Accuracy: 57.09, Validation Loss: 0.01459, Validation Accuracy: 51.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axtnp0ROU-f9",
        "colab_type": "text"
      },
      "source": [
        "### Calculate performance of the trained model (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGwZEnAKU-f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, testset):\n",
        "    model.eval()\n",
        "    accuracies, f1_scores, precisions, recalls = [], [], [], []\n",
        "    for batch in testset:\n",
        "        with torch.no_grad():\n",
        "            x = batch.text[0]\n",
        "            y = batch.label\n",
        "            preds = model(x)\n",
        "            n_correct = (torch.max(preds, 1)[1].view(y.size()).data == y.data).float().sum().detach().cpu().numpy()\n",
        "            accuracies.append(100.0 * n_correct/len(batch))\n",
        "            logit = torch.max(preds, 1)[1].view(y.size()).data.detach().cpu().numpy()\n",
        "            target = y.data.detach().cpu().numpy()\n",
        "            f1_scores.append(f1_score(logit, target, zero_division=1))\n",
        "            precisions.append(precision_score(logit, target, zero_division=0))\n",
        "            recalls.append(recall_score(logit, target, zero_division=1))\n",
        "\n",
        "    print('Average classification scores:\\nAccuracy:{}, \\nPrecision:{},\\nRecall:{},\\nF1-score:{}'.format(round(np.mean(accuracies),3),\n",
        "                                                                                                     round(100.0 *np.mean(f1_scores),3),\n",
        "                                                                                                     round(100.0 *np.mean(precisions),3),\n",
        "                                                                                                     round(100.0 *np.mean(recalls),3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmE77j5cx9Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_model(model, test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgw7ypkwU-f_",
        "colab_type": "text"
      },
      "source": [
        "Write down the calculated performance\n",
        "\n",
        "### Accuracy: 51.219\n",
        "### Precision: 16.324\n",
        "### Recall: 9.858\n",
        "### F1: 49.957"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp_AZKhpU-gA",
        "colab_type": "text"
      },
      "source": [
        "### Experiments (5 points)\n",
        "\n",
        "Experiment with the model and achieve better results. Implement and describe your experiments in details, mention what was helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH-Fgwr3U-gA",
        "colab_type": "text"
      },
      "source": [
        "### 1. Bad results on the baseline. Let's tune the model. Actions: lower the embedding dimension, lower the learning rate, increase the number of epochs.Results: Accuracy:70.827, Precision:42.507, Recall:37.075, F1-score:50.031"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3LljShDwDiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kernel_sizes = [3,4,5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 256\n",
        "\n",
        "model = CNN(vocab_size, dim, kernel_sizes, dropout)\n",
        "model.cuda()\n",
        "opt = optim.Adam(model.parameters(), lr=0.0002)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3TvIc5JyHiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "8f5a7616-89e1-4460-9d4c-08797371997f"
      },
      "source": [
        "train_model(model, opt, loss_func, 30, train_iter, val_iter)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
            "Wall time: 7.87 µs\n",
            "Epoch: 1, Training Loss: 0.01175, Training Accuracy: 50.48, Validation Loss: 0.01079, Validation Accuracy: 53.44\n",
            "Epoch: 2, Training Loss: 0.01079, Training Accuracy: 53.86, Validation Loss: 0.01041, Validation Accuracy: 62.01\n",
            "Epoch: 3, Training Loss: 0.01056, Training Accuracy: 55.34, Validation Loss: 0.01028, Validation Accuracy: 63.45\n",
            "Epoch: 4, Training Loss: 0.01042, Training Accuracy: 57.42, Validation Loss: 0.01017, Validation Accuracy: 64.83\n",
            "Epoch: 5, Training Loss: 0.01037, Training Accuracy: 58.3, Validation Loss: 0.01012, Validation Accuracy: 65.34\n",
            "Epoch: 6, Training Loss: 0.01023, Training Accuracy: 60.33, Validation Loss: 0.01005, Validation Accuracy: 65.49\n",
            "Epoch: 7, Training Loss: 0.01019, Training Accuracy: 60.65, Validation Loss: 0.00996, Validation Accuracy: 66.46\n",
            "Epoch: 8, Training Loss: 0.01008, Training Accuracy: 61.68, Validation Loss: 0.00987, Validation Accuracy: 66.96\n",
            "Epoch: 9, Training Loss: 0.00995, Training Accuracy: 63.13, Validation Loss: 0.00979, Validation Accuracy: 67.28\n",
            "Epoch: 10, Training Loss: 0.00989, Training Accuracy: 64.44, Validation Loss: 0.00974, Validation Accuracy: 67.44\n",
            "Epoch: 11, Training Loss: 0.00984, Training Accuracy: 64.8, Validation Loss: 0.00964, Validation Accuracy: 68.41\n",
            "Epoch: 12, Training Loss: 0.00966, Training Accuracy: 66.02, Validation Loss: 0.00958, Validation Accuracy: 68.49\n",
            "Epoch: 13, Training Loss: 0.00959, Training Accuracy: 67.11, Validation Loss: 0.00953, Validation Accuracy: 68.77\n",
            "Epoch: 14, Training Loss: 0.00946, Training Accuracy: 68.04, Validation Loss: 0.00945, Validation Accuracy: 68.56\n",
            "Epoch: 15, Training Loss: 0.00932, Training Accuracy: 68.41, Validation Loss: 0.00938, Validation Accuracy: 69.29\n",
            "Epoch: 16, Training Loss: 0.00921, Training Accuracy: 69.52, Validation Loss: 0.00934, Validation Accuracy: 70.08\n",
            "Epoch: 17, Training Loss: 0.00913, Training Accuracy: 70.07, Validation Loss: 0.00928, Validation Accuracy: 69.99\n",
            "Epoch: 18, Training Loss: 0.00894, Training Accuracy: 71.43, Validation Loss: 0.00925, Validation Accuracy: 70.47\n",
            "Epoch: 19, Training Loss: 0.00888, Training Accuracy: 71.98, Validation Loss: 0.00925, Validation Accuracy: 69.79\n",
            "Epoch: 20, Training Loss: 0.00874, Training Accuracy: 72.78, Validation Loss: 0.00917, Validation Accuracy: 70.13\n",
            "Epoch: 21, Training Loss: 0.00859, Training Accuracy: 73.3, Validation Loss: 0.00914, Validation Accuracy: 70.01\n",
            "Epoch: 22, Training Loss: 0.00842, Training Accuracy: 74.52, Validation Loss: 0.00908, Validation Accuracy: 70.06\n",
            "Epoch: 23, Training Loss: 0.00834, Training Accuracy: 74.53, Validation Loss: 0.00907, Validation Accuracy: 70.58\n",
            "Epoch: 24, Training Loss: 0.00824, Training Accuracy: 74.91, Validation Loss: 0.00903, Validation Accuracy: 70.33\n",
            "Epoch: 25, Training Loss: 0.00807, Training Accuracy: 75.8, Validation Loss: 0.00901, Validation Accuracy: 70.5\n",
            "Epoch: 26, Training Loss: 0.00795, Training Accuracy: 76.26, Validation Loss: 0.00898, Validation Accuracy: 70.46\n",
            "Epoch: 27, Training Loss: 0.00778, Training Accuracy: 76.75, Validation Loss: 0.00896, Validation Accuracy: 70.68\n",
            "Epoch: 28, Training Loss: 0.00762, Training Accuracy: 77.5, Validation Loss: 0.00894, Validation Accuracy: 70.62\n",
            "Epoch: 29, Training Loss: 0.00758, Training Accuracy: 77.78, Validation Loss: 0.00893, Validation Accuracy: 70.72\n",
            "Epoch: 30, Training Loss: 0.00751, Training Accuracy: 77.81, Validation Loss: 0.00894, Validation Accuracy: 70.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvJZVvjByHvy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f4c26e06-3a41-4171-e030-f3c299034c9c"
      },
      "source": [
        "evaluate_model(model, test_iter)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average classification scores:\n",
            "Accuracy:70.827, \n",
            "Precision:42.507,\n",
            "Recall:37.075,\n",
            "F1-score:50.031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtYDn8jW2ZWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}